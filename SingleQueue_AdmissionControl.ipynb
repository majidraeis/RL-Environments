{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SingleQueue_AdmissionControl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGud7vECcGH9tAhU1jTxsI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw1TTQhCSIeZ",
        "colab_type": "text"
      },
      "source": [
        "# An Environment for Admission Control in Single-stage Multi-server Queueing System\n",
        "\n",
        "Consider a multi-server queueing system with $c$ servers. We want to learn an admission control policy that stabilizes the queue such that the average backlog converges to some given value (e.g. $B_{th}$).\n",
        "## Goal:\n",
        "Find $\\pi$ such that $E_{\\pi}[b(t)]\\to B_{th}$,\n",
        "where $b(t)$ denotes the backlog at time $t$.\n",
        "\n",
        "## State (s):\n",
        "Number of jobs in the system (queue+servers) upon a job arrival\n",
        "## Actions (a):\n",
        "Accept or Reject the incoming job\n",
        "## Reward ( R(s,a) ):\n",
        "$b$ denotes backlog ($b = \\max(s - c, 0 )$) \n",
        ">\n",
        "$\n",
        "\\begin{equation*}\n",
        "r_1 = \\left\\{\n",
        "\\begin{array}{ll}\n",
        "1,  \\qquad & b <B_{th}\\\\ \n",
        "-1,  \\qquad & b >B_{th}\n",
        "\\end{array} \\right.\n",
        "\\end{equation*}\n",
        "\\qquad$\n",
        "$\n",
        "\\begin{equation*}\n",
        "r_2 = \\left\\{\n",
        "\\begin{array}{ll}\n",
        "1,  \\qquad & a = Accept\\\\ \n",
        "-1,  \\qquad & a = Reject\n",
        "\\end{array} \\right.\n",
        "\\end{equation*}\n",
        "$\n",
        "\n",
        "$R(s,a) = r_1.r_2$\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/majidraeis/Figs/master/queue_demo.gif\" width=\"600\" height=\"400\" />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFYm4nEUaS0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import tkinter as tk\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('TkAgg')\n",
        "from matplotlib import style\n",
        "from matplotlib.figure import Figure\n",
        "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
        "\n",
        "style.use(\"ggplot\")\n",
        "job_color = '#191970'\n",
        "line_color = '#00BFFF'\n",
        "\n",
        "class QueueEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Define a multi-server queue.\n",
        "    The environment defines the admission control problem in a multi-server queue.\n",
        "    Action: accept(1) or reject(0), State: 1-queue length 2-#of busy servers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_s, rho, B_th):\n",
        "        #         self.__version__ = \"0.1.0\"\n",
        "        # General variables defining the environment\n",
        "        self.n_servers = n_s\n",
        "        self.rho = rho\n",
        "        self.B_th = B_th\n",
        "        self.n_jobs = 0\n",
        "        self.ql_vec = [0]\n",
        "        self.t_arr = 0  # First arrival time is 0 by default\n",
        "        self.t_vec = [0.0]\n",
        "        self.render_initiate = True\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Discrete(1000)\n",
        "        self.empty_servers = np.arange(n_s)\n",
        "        self.assigned_servers = []\n",
        "        self.t_fin = []\n",
        "        self.job_dict = {}\n",
        "        self.job_dict[0] = {'Tw': 0.0}\n",
        "        self.accepted_job_ind_vec = []\n",
        "        self.waiting_vec = []\n",
        "        self.job_index = 1\n",
        "        self.cnt = 1\n",
        "        self.MAX_STEPS = 10000\n",
        "        self.last_entered_job = 0\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        self._take_action(action)\n",
        "        reward = self._get_reward(action)\n",
        "        ob = self.n_jobs\n",
        "        done = False if self.cnt < self.MAX_STEPS else True\n",
        "        self.cnt += 1\n",
        "        return ob, reward, done, {}\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        # ----Queue length before taking the action (upon job arrival)---------\n",
        "        self.ql = max(self.n_jobs - self.n_servers, 0)\n",
        "        if action:\n",
        "            if self.n_jobs < self.n_servers:\n",
        "                t_ent = self.t_arr\n",
        "                self.empty_servers = [x for x in range(self.n_servers) if x not in self.assigned_servers]\n",
        "                self.assigned_servers = np.append(self.assigned_servers, random.choice(self.empty_servers))\n",
        "\n",
        "            else:\n",
        "                # ------finding the time that each server gets empty------------\n",
        "                t_available = [np.max(self.t_fin[self.assigned_servers == i]) for i in range(self.n_servers)]\n",
        "                # -----------pick the earliest server available-----------------\n",
        "                picked_server = np.argmin(t_available)\n",
        "                t_ent = max(self.t_arr, t_available[picked_server])\n",
        "                self.assigned_servers = np.append(self.assigned_servers, picked_server)\n",
        "\n",
        "            t_s = self._service_gen()\n",
        "            self.t_fin = np.append(self.t_fin, t_ent + t_s)\n",
        "            self.n_jobs += 1\n",
        "            self.job_dict[self.job_index] = {'Ta': self.t_arr, 'Td': t_ent + t_s, 'Ts': t_s, 'Tw': t_ent-self.t_arr,\n",
        "                                             'Ba': self.ql}\n",
        "            self.last_entered_job = self.job_index\n",
        "\n",
        "        self.last_t_arr = self.t_arr\n",
        "        self.t_arr += self._inter_arr_gen()\n",
        "        served_jobs_ind = np.arange(len(self.t_fin))[np.array(self.t_fin) < self.t_arr]\n",
        "        if len(np.array(env.t_fin) < env.t_arr):\n",
        "            self.last_t_fins = self.t_fin[np.array(self.t_fin) < self.t_arr]\n",
        "        else:\n",
        "            self.last_t_fins = []\n",
        "        self.n_jobs -= np.sum(np.array(self.t_fin) < self.t_arr)\n",
        "        self.t_fin = np.delete(self.t_fin, served_jobs_ind)\n",
        "        self.assigned_servers = np.delete(self.assigned_servers, served_jobs_ind)\n",
        "        self.job_index += 1\n",
        "\n",
        "    def _inter_arr_gen(self):\n",
        "        lambda_a = self.n_servers * self.rho\n",
        "        return np.random.exponential(1 / lambda_a)\n",
        "\n",
        "    def _service_gen(self):\n",
        "        lambda_s = 1.0\n",
        "        return np.random.exponential(1 / lambda_s)\n",
        "\n",
        "    def _get_reward(self, action):\n",
        "        # --Queue length after taking the action (right before the next arrival)--\n",
        "        self.ql = max(self.n_jobs - self.n_servers, 0)\n",
        "        if action:\n",
        "            if self.ql > self.B_th:\n",
        "                return -1000\n",
        "            else:\n",
        "                return 5\n",
        "        else:\n",
        "            return -5\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        self.ql_vec = [0]\n",
        "        self.n_jobs = 0\n",
        "        self.t_arr = 0  # ---------First arrival time is 0 by default-----------\n",
        "        self.t_vec = [0]\n",
        "        self.empty_servers = np.arange(self.n_servers)\n",
        "        self.assigned_servers = []\n",
        "        self.t_fin = []\n",
        "        self.job_dict = {}\n",
        "        self.job_index = 1\n",
        "        self.accepted_job_ind_vec = []\n",
        "        self.waiting_vec = []\n",
        "        self.job_dict[0] = {'Tw': 0.0}\n",
        "        self.last_entered_job = 0\n",
        "        return self.n_jobs\n",
        "\n",
        "    def _text(self):\n",
        "        text = \"QL = %d\" % self.ql\n",
        "        self.canvas.itemconfig(self.text, text=text)\n",
        "        self.canvas.update\n",
        "    # --------------rendering the queue after taking the action-----------------\n",
        "    def render(self, mode='human', close=False):  \n",
        "\n",
        "        self._Queue_plot()\n",
        "        self._QL_plot()\n",
        "        self._waiting_plot()\n",
        "        self._text()\n",
        "        return\n",
        "\n",
        "    def _Queue_plot(self):\n",
        "\n",
        "        num_busy_servers = max(self.n_jobs - self.ql, 0)\n",
        "        qHead    = 300\n",
        "        qTail.   = 150\n",
        "        p2qDist  = 2\n",
        "        c2cDist  = 4\n",
        "        shift_up = 70\n",
        "        cirR_o   = 25  # Server circle outer radius\n",
        "        cirR_i   = 22  # Server circle inner radius\n",
        "        cirC     = [335, 200]\n",
        "\n",
        "        linewidth = 2\n",
        "        cirC[1] = cirC[1] - ((self.n_servers - 1) * cirR_o + (self.n_servers - 1) * c2cDist / 2)\n",
        "\n",
        "        if self.render_initiate:\n",
        "            self.root = tk.Tk()\n",
        "            self.canvas = tk.Canvas(self.root, width=550, height=250)\n",
        "            self.root.title('Queueing')\n",
        "            self.canvas.pack()\n",
        "            self.inner_circle = []\n",
        "            self.canvas.create_line(qTail, 175-shift_up, qHead, 175-shift_up, width=linewidth, fill=line_color)\n",
        "            self.canvas.create_line(qTail, 225-shift_up, qHead, 225-shift_up, width=linewidth, fill=line_color)\n",
        "            self.canvas.create_line(qHead, 175-shift_up, qHead, 225-shift_up, width=linewidth, fill=line_color)\n",
        "            for c in range(self.n_servers):\n",
        "                self.canvas.create_oval(cirC[0] - cirR_o, cirC[1] - cirR_o-shift_up, cirC[0] + cirR_o,\n",
        "                                        cirC[1] + cirR_o-shift_up, outline=line_color, width=linewidth)\n",
        "                self.inner_circle.append(self.canvas.create_oval(cirC[0] - cirR_i, cirC[1] - cirR_i-shift_up, cirC[0]\n",
        "                                         + cirR_i, cirC[1] + cirR_i-shift_up, outline='white',\n",
        "                                         fill='white', width=linewidth))\n",
        "                cirC[1] = cirC[1] + c2cDist + 2 * cirR_o\n",
        "            self.queue_len = self.canvas.create_rectangle(qHead - p2qDist-p2qDist, 180-shift_up, qHead - p2qDist-p2qDist\n",
        "                                                          , 220-shift_up, fill=job_color)\n",
        "\n",
        "            self.text = self.canvas.create_text((qTail+qHead)/2, 166-shift_up, fill=\"black\", text=\"Queue length = 0\")\n",
        "        else:\n",
        "\n",
        "            self.canvas.coords(self.queue_len, qHead - p2qDist-p2qDist - self.ql * 5, 180-shift_up,\n",
        "                               qHead - p2qDist-p2qDist, 220-shift_up)\n",
        "\n",
        "            for c in range(self.n_servers):\n",
        "                if c+1 <= num_busy_servers:\n",
        "                    self.canvas.itemconfig(self.inner_circle[c], fill=job_color)\n",
        "                    self.canvas.update()\n",
        "\n",
        "                else:\n",
        "                    self.canvas.itemconfig(self.inner_circle[c], fill='white', width=linewidth)\n",
        "                    self.canvas.update()\n",
        "\n",
        "                cirC[1] = cirC[1] + c2cDist + 2 * cirR_o\n",
        "\n",
        "    def _QL_plot(self):\n",
        "        # **************After taking the action*************\n",
        "        # ***************** Queue Length *******************\n",
        "        n_job_after_action = self.n_jobs + len(self.last_t_fins)\n",
        "        ql_after_action = max(n_job_after_action - self.n_servers, 0)\n",
        "        self.ql_vec = np.append(self.ql_vec, ql_after_action)\n",
        "        self.t_vec = np.append(self.t_vec, self.last_t_arr)\n",
        "        # ******** Departures until the next arrival **********\n",
        "        ql_deps = n_job_after_action - np.arange(1, len(self.last_t_fins)+1)\n",
        "        ql_deps = (ql_deps - self.n_servers) * (ql_deps - self.n_servers > 0)\n",
        "        self.ql_vec = np.append(self.ql_vec, ql_deps)\n",
        "        self.t_vec = np.append(self.t_vec, np.sort(self.last_t_fins))\n",
        "\n",
        "        if self.render_initiate:\n",
        "\n",
        "            self.figure1 = Figure(figsize=(5, 4))\n",
        "            self.subplot1 = self.figure1.add_subplot(111)\n",
        "            # --------Queue length after taking the action---------------------\n",
        "            self.subplot1.step(self.t_vec, self.ql_vec, where='post', color='lightsteelblue')\n",
        "            self.plot1 = FigureCanvasTkAgg(self.figure1, self.root)\n",
        "            self.plot1.get_tk_widget().pack(side=tk.LEFT, fill=tk.BOTH, expand=1)\n",
        "            self.subplot1.set_xlabel('Time')\n",
        "            self.subplot1.set_ylabel('Queue length')\n",
        "        else:\n",
        "\n",
        "            self.subplot1.clear()\n",
        "            self.subplot1.step(self.t_vec, self.ql_vec, where='post', color='tab:blue')\n",
        "            self.figure1.canvas.draw_idle()\n",
        "            self.subplot1.set_xlabel('Time')\n",
        "            self.subplot1.set_ylabel('Queue length')\n",
        "\n",
        "    def _waiting_plot(self):\n",
        "        # ***************** Waiting time ********************\n",
        "        if self.render_initiate:\n",
        "\n",
        "            self.accepted_job_ind_vec.append(self.last_entered_job)\n",
        "            self.waiting_vec.append(self.job_dict[self.last_entered_job]['Tw'])\n",
        "            self.figure2 = Figure(figsize=(5, 4))\n",
        "            self.subplot2 = self.figure2.add_subplot(111)\n",
        "            self.subplot2.plot(self.accepted_job_ind_vec, self.waiting_vec, color='tab:red')\n",
        "            self.subplot2.set_xlabel('Job')\n",
        "            self.subplot2.set_ylabel('Waiting time')\n",
        "            self.plot2 = FigureCanvasTkAgg(self.figure2, env.root)\n",
        "            self.plot2.get_tk_widget().pack(side=tk.LEFT, fill=tk.BOTH, expand=1)\n",
        "            self.render_initiate = False  # This must be run in the last plot function\n",
        "        else:\n",
        "            self.accepted_job_ind_vec.append(self.last_entered_job)\n",
        "            self.waiting_vec.append(self.job_dict[self.last_entered_job]['Tw'])\n",
        "            self.subplot2.clear()\n",
        "            self.subplot2.plot(self.accepted_job_ind_vec, self.waiting_vec, color='tab:red')\n",
        "            self.subplot2.set_xlabel('Job')\n",
        "            self.subplot2.set_ylabel('Waiting time')\n",
        "            self.figure2.canvas.draw_idle()\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, env):\n",
        "        self.isCont = \\\n",
        "            type(env.action_space) == gym.spaces.box.Box\n",
        "        if self.isCont:\n",
        "            self.action_low = env.action_space.low\n",
        "            self.action_high = env.action_space.high\n",
        "            self.action_shape = env.action_space.shape\n",
        "\n",
        "        else:\n",
        "            self.action_size = env.action_space.n\n",
        "            print(\"Action size:\", self.action_size)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if self.isCont:\n",
        "            action = np.random.uniform(self.action_low, self.action_high, self.action_shape)\n",
        "        else:\n",
        "            action = random.choice(range(env.action_space.n))\n",
        "        return action\n",
        "\n",
        "class QAgent(Agent):\n",
        "    def __init__(self, env, discount_rate=0.97, learning_rate=0.01):\n",
        "        super().__init__(env)\n",
        "        self.state_size = env.observation_space.n\n",
        "\n",
        "        self.discount_rate = discount_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.eps = 0.98\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        self.q_table = 1e-4 * np.random.random((self.state_size, self.action_size))\n",
        "\n",
        "    def get_action(self, state):\n",
        "        q_state = self.q_table[state]\n",
        "        action_greedy = np.argmax(q_state)\n",
        "        action_random = super().get_action(state)\n",
        "        return action_random if random.random() < self.eps else action_greedy\n",
        "\n",
        "    def train(self, experience):\n",
        "        state, action, next_state, reward, done = experience\n",
        "\n",
        "        q_next = self.q_table[next_state]\n",
        "        q_next = np.zeros([self.action_size]) if done else q_next\n",
        "        q_target = reward + self.discount_rate * np.max(q_next)\n",
        "\n",
        "        q_update = q_target - self.q_table[state, action]\n",
        "        self.q_table[state, action] += self.learning_rate * q_update\n",
        "\n",
        "        if done:\n",
        "            self.eps *= 0.99\n",
        "\n",
        "\n",
        "#******************  Demo  ********************\n",
        "#**********************************************\n",
        "\n",
        "render_flag = True\n",
        "env = QueueEnv(3, 3, 10)\n",
        "agent = QAgent(env)\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    agent.train((state, action, next_state, reward, done))\n",
        "    state = next_state\n",
        "    print(\"s:\", state, \"a:\", action)\n",
        "    if render_flag:\n",
        "        env.render()\n",
        "\n",
        "if render_flag:\n",
        "    env.canvas.mainloop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}